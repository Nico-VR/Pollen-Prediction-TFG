{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e97e0e2-e784-408b-b479-1f60d133139f",
   "metadata": {},
   "source": [
    "## 3 Evaluate and benchmark\n",
    "\n",
    "In the following notebooks we will evaluate the predictions of all models by evaluating how well estimate the **Main Pollen Season** (MPS) first. To obtain the MPS, we use two alternative formulas: a percentage-based approach and a threshold-based approach. These functions will be used to compare the predicted MPS periods against the true observed (real) values, and the results will be summarized in a comparison table.\n",
    "\n",
    "The evaluation generates a CSV file with metrics, which will serve as the basis for a comparison (benchmark).\n",
    "\n",
    "Then we will compare the performance of all models and their variants using this evaluation CSV. \n",
    "\n",
    "### 3.1. Percentage formula\n",
    "\n",
    "The **main pollen season** (MPS) was determined by including the central 95% of the annual pollen count. Specifically, it began when 2.5% of the total yearly pollen had accumulated and ended when it reached 97.5%. This method preserves most of the relevant data while excluding the early and late outliers, which helps improve the performance of machine learning models.\n",
    "### 3.2. Threshold formula\n",
    "\n",
    "The **main pollen season** (MPS) can also be determined using a threshold-based method. In this case, the season was defined based on two parameters: a minimum daily pollen count (threshold) and a required number of consecutive days meeting that threshold. Specifically, the MPS was considered to begin when at least three pollen grains were recorded on three consecutive days. The season ended when this condition was no longer met in the subsequent data.\n",
    "\n",
    "### 3.3. Evaluation CSV Example\n",
    "The resulting table after obtaining a evaluation from the predictions has the following format in which it is displayed:\n",
    "\n",
    "| year | model_name | uses_covariates | train_size | pred_n | start | end | est_start | est_end | start_dev | end_dev | duration | est_duration | total_dev | duration_dev |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "| The year in which the MPS is being forecast  | Name of the model | Whether covariates are used for predictions (True) or not (False)| The number of days with which the model is trained | The n days before the target date predictions | Real start day of MPS | Real end day of MPS | Estimated start day of MPS | Estimated end day of MPS | Deviation in days between the actual start date and the estimated start date | Deviation in days between the actual end date and the estimated end date | Number of days of MPS | Estimated number of days of MPS | Sum of the deviations start_dev and end_dev | Difference between estimated and real MPS duration | \n",
    "\n",
    "For example, if ```HORIZON_SIZE``` of 1 week, 0 years of ```TRAIN_SIZE```(fitting),```START_YEAR = 1999```, ```END_YEAR = 2010```, ```OFFSET_DAYS = -182```, does not predict using covariates and the moirai.deterministic.small model was used, the evaluation will be like this: \n",
    "```\n",
    "year,model_name,uses_covariates,train_size,pred_n,start,end,est_start,est_end,start_dev,end_dev,duration,est_duration,total_dev,duration_dev\n",
    "1999,moirai.deterministic.small,False,0,1,1999-01-13,1999-02-27,1998-08-11,1999-12-17,155,293,45,493,448,-448\n",
    "1999,moirai.deterministic.small,False,0,2,1999-01-13,1999-02-27,1998-08-09,1999-12-18,157,294,45,496,451,-451\n",
    "1999,moirai.deterministic.small,False,0,3,1999-01-13,1999-02-27,1998-09-21,1999-12-06,114,282,45,441,396,-396\n",
    "1999,moirai.deterministic.small,False,0,4,1999-01-13,1999-02-27,1998-08-12,1999-12-18,154,294,45,493,448,-448\n",
    "1999,moirai.deterministic.small,False,0,5,1999-01-13,1999-02-27,1998-09-28,1999-11-28,107,274,45,426,381,-381\n",
    "1999,moirai.deterministic.small,False,0,6,1999-01-13,1999-02-27,1998-08-15,1999-12-13,151,289,45,485,440,-440\n",
    "1999,moirai.deterministic.small,False,0,7,1999-01-13,1999-02-27,1998-08-17,1999-12-14,149,290,45,484,439,-439\n",
    "...\n",
    "2010,moirai.deterministic.small,False,0,1,2010-01-18,2010-03-05,2009-07-24,2010-12-09,178,279,46,503,457,-457\n",
    "2010,moirai.deterministic.small,False,0,2,2010-01-18,2010-03-05,2009-08-01,2010-12-13,170,283,46,499,453,-453\n",
    "2010,moirai.deterministic.small,False,0,3,2010-01-18,2010-03-05,2009-09-12,2010-11-16,128,256,46,430,384,-384\n",
    "2010,moirai.deterministic.small,False,0,4,2010-01-18,2010-03-05,2009-07-20,2010-12-11,182,281,46,509,463,-463\n",
    "2010,moirai.deterministic.small,False,0,5,2010-01-18,2010-03-05,2009-09-04,2010-11-15,136,255,46,437,391,-391\n",
    "2010,moirai.deterministic.small,False,0,6,2010-01-18,2010-03-05,2009-08-01,2010-12-06,170,276,46,492,446,-446\n",
    "2010,moirai.deterministic.small,False,0,7,2010-01-18,2010-03-05,2009-08-01,2010-12-04,170,274,46,490,444,-444\n",
    "```\n",
    "\n",
    "The file name will be as follows: \n",
    "```\n",
    "suffix = \"percentage\" if apply_percentage else \"threshold\"        \n",
    "    if apply_percentage:\n",
    "        file_path = os.path.join( evaluation_dir,\n",
    "            f\"mps_evaluation_{suffix}_{start_year}-{end_year}_{offset_days}_{input_size}_{horizon_size}_{start_pct}_{end_pct}.csv\")\n",
    "    else:\n",
    "        file_path = os.path.join( evaluation_dir,\n",
    "            f\"mps_evaluation_{suffix}_{start_year}-{end_year}_{offset_days}_{input_size}_{horizon_size}_{threshold}_{consecutive_days}.csv\")\n",
    "```\n",
    "\n",
    "For example, if ```evaluation_dir``` is: *./outputs/evaluations/*, ```apply_percentage``` is: *True*, ```start_pct``` is: *0.025*, ```end_pct``` is: *0.975* then ```file_path``` will be *./outputs/evaluations/mps_evaluation_percentage_1999-2010_-182_365_7_0.025_0.975.csv*.\n",
    "\n",
    "### 3.4. Benchmark the models\n",
    "\n",
    "In the last notebook, we will compare the performance of the models. The metrics we will use for the benchmark will be the ```start_dev``` and ```end_dev```.\n",
    "\n",
    "The first thing to do is extracting the metrics of the CSVs: ```start_dev```, ```end_dev```. We will then make a box and whisker plot and then a comparative table of model results after running Mann-Whitney U with Holm-Bonferroni correction, to compare the performance of all models in obtaining the MPS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a1e0b-f0b5-4662-8f35-17e599103c25",
   "metadata": {},
   "source": [
    "➡️ **[Next notebook: 03-01_Evaluate_models](../notebooks/03-01_Evaluate_models.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
